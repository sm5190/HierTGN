{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqZowJyIGKr-"
   },
   "source": [
    "# **This code is working**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEMxwYltiZc-",
    "outputId": "c24b6e8f-aa2d-4ea2-ff1e-daa8cf9c908e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: torch 2.4.0\n",
      "Uninstalling torch-2.4.0:\n",
      "  Successfully uninstalled torch-2.4.0\n",
      "Collecting torch==2.4\n",
      "  Using cached torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (1.12)\n",
      "Requirement already satisfied: networkx in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch==2.4) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/melike/anaconda3/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.4) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/melike/anaconda3/lib/python3.12/site-packages (from sympy->torch==2.4) (1.3.0)\n",
      "Using cached torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.4.0\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
      "Requirement already satisfied: pyg_lib in /home/melike/anaconda3/lib/python3.12/site-packages (0.4.0+pt24cu121)\n",
      "Requirement already satisfied: torch_scatter in /home/melike/anaconda3/lib/python3.12/site-packages (2.1.2+pt24cu121)\n",
      "Requirement already satisfied: torch_sparse in /home/melike/anaconda3/lib/python3.12/site-packages (0.6.18+pt24cu121)\n",
      "Requirement already satisfied: torch_cluster in /home/melike/anaconda3/lib/python3.12/site-packages (1.6.3+pt24cu121)\n",
      "Requirement already satisfied: torch_spline_conv in /home/melike/anaconda3/lib/python3.12/site-packages (1.2.2+pt24cu121)\n",
      "Requirement already satisfied: scipy in /home/melike/anaconda3/lib/python3.12/site-packages (from torch_sparse) (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /home/melike/anaconda3/lib/python3.12/site-packages (from scipy->torch_sparse) (1.26.4)\n",
      "Requirement already satisfied: torch-geometric in /home/melike/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (3.9.5)\n",
      "Requirement already satisfied: fsspec in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /home/melike/anaconda3/lib/python3.12/site-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/melike/anaconda3/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from aiohttp->torch-geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/melike/anaconda3/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/melike/anaconda3/lib/python3.12/site-packages (from aiohttp->torch-geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from aiohttp->torch-geometric) (1.9.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from jinja2->torch-geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/melike/anaconda3/lib/python3.12/site-packages (from requests->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/melike/anaconda3/lib/python3.12/site-packages (from requests->torch-geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/melike/anaconda3/lib/python3.12/site-packages (from requests->torch-geometric) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/melike/anaconda3/lib/python3.12/site-packages (from requests->torch-geometric) (2024.7.4)\n",
      "Requirement already satisfied: scikit-learn in /home/melike/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/melike/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/melike/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/melike/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/melike/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!find /usr/local/lib/python3.* -type d -name __pycache__ -exec rm -r {} +\n",
    "!find . -type d -name __pycache__ -exec rm -r {} +\n",
    "\n",
    "\n",
    "! pip uninstall torchaudio torchvision --y\n",
    "\n",
    "! pip uninstall torch --y\n",
    "! pip install torch==2.4\n",
    "! pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
    "! pip install torch-geometric\n",
    "! pip install scikit-learn\n",
    "! pip install matplotlib\n",
    "\n",
    "# import torch\n",
    "\n",
    "# print(torch.__version__)\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gKTracICidsG"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryDataset, TemporalData, download_url\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mICEWS14Dataset\u001b[39;00m(InMemoryDataset):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset, TemporalData, download_url\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class ICEWS14Dataset(InMemoryDataset):\n",
    "    r\"\"\"The temporal knowledge graph dataset from the ICEWS14 benchmark dataset,\n",
    "    which combines train, validation, and test files into a single dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before being saved to\n",
    "            disk. (default: :obj:`None`)\n",
    "        force_reload (bool, optional): Whether to re-process the dataset.\n",
    "            (default: :obj:`False`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - Name\n",
    "          - #entities\n",
    "          - #relations\n",
    "          - #events\n",
    "        * - ICEWS14\n",
    "          - varies\n",
    "          - varies\n",
    "          - varies\n",
    "    \"\"\"\n",
    "    url = 'https://github.com/INK-USC/RE-Net/raw/master/data/ICEWS14'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        force_reload: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(root, transform, pre_transform, force_reload=force_reload)\n",
    "        self.load(self.processed_paths[0], data_cls=TemporalData)\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, 'ICEWS14', 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, 'ICEWS14', 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list:\n",
    "        # List of files to download and process\n",
    "        return ['train.txt',  'test.txt'] #'valid.txt',\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self) -> None:\n",
    "        # Download each file separately\n",
    "        for file_name in self.raw_file_names:\n",
    "            download_url(f\"{self.url}/{file_name}\", self.raw_dir)\n",
    "\n",
    "    def process(self) -> None:\n",
    "\n",
    "        # List to hold DataFrames for each file\n",
    "        dfs = []\n",
    "            # List to hold DataFrames for each file\n",
    "\n",
    "        for file_name in self.raw_file_names:\n",
    "            file_path = osp.join(self.raw_dir, file_name)\n",
    "            df = pd.read_csv(file_path, sep='\\t', header=None, usecols=[0, 1, 2, 3],\n",
    "                            names=['src', 'relation', 'dst', 'timestamp'])\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "            # Draw histograms for each column\n",
    "          # columns = ['src', 'relation', 'dst', 'timestamp']\n",
    "\n",
    "          # for column in columns:\n",
    "          #     plt.figure(figsize=(8, 5))\n",
    "          #     plt.hist(df[column], bins=30, alpha=0.7, edgecolor='black')\n",
    "          #     plt.title(f'Histogram of {column}')\n",
    "          #     plt.xlabel(column)\n",
    "          #     plt.ylabel('Frequency')\n",
    "          #     plt.grid(axis='y', alpha=0.75)\n",
    "          #     plt.show()\n",
    "\n",
    "        # Concatenate all dataframes into a single dataframe\n",
    "        full_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Total events (triples) in combined dataset: {len(full_df)}\")\n",
    "\n",
    "        # Process columns for temporal data\n",
    "        src = torch.tensor(full_df['src'].values, dtype=torch.long)\n",
    "        dst = torch.tensor(full_df['dst'].values, dtype=torch.long)\n",
    "        #relation = torch.tensor(full_df['relation'].values, dtype=torch.float).tolist()\n",
    "        relation = torch.tensor(full_df['relation'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        timestamp = torch.tensor(full_df['timestamp'].values, dtype=torch.long)\n",
    "\n",
    "        print(\"\\nTensor shapes:\")\n",
    "        print(f\"src: {src.shape}\")\n",
    "        print(f\"dst: {dst.shape}\")\n",
    "        print(f\"t: {timestamp.shape}\")\n",
    "        print(f\"msg: {relation.shape}\")\n",
    "\n",
    "        # Create the TemporalData object\n",
    "        data = TemporalData(src=src, dst=dst, t=timestamp, msg=relation)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        # Print debug information about the data object\n",
    "        print(\"Data object before saving:\")\n",
    "        print(f\"  - Number of source nodes (src): {data.src.size(0)}\")\n",
    "        print(f\"  - Number of destination nodes (dst): {data.dst.size(0)}\")\n",
    "        print(f\"  - Number of timestamps (t): {data.t.size(0)}\")\n",
    "        print(f\"  - Number of relations (msg): {data.msg.size(0)}\")\n",
    "\n",
    "        # Save the processed data\n",
    "        self.save([data], self.processed_paths[0])\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return 'ICEWS14()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwsY1vl7ierh",
    "outputId": "9ffcaef8-ebc3-4ea1-e16a-7e310d3c888f"
   },
   "outputs": [],
   "source": [
    "dataset = ICEWS14Dataset(root='./data/')\n",
    "data = dataset[0]  # Access the single TemporalData object\n",
    "\n",
    "# Calculate statistics based on the TemporalData object directly\n",
    "num_events = data.src.size(0)\n",
    "unique_src_nodes = set(data.src.tolist())\n",
    "unique_dst_nodes = set(data.dst.tolist())\n",
    "# unique_relations = set(data.msg.tolist())\n",
    "unique_timestamps = set(data.t.tolist())\n",
    "unique_nodes = unique_src_nodes.union(unique_dst_nodes)\n",
    "num_nodes = len(unique_nodes)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Number of events (triples): {num_events}\")\n",
    "print(f\"Number of unique nodes: {num_nodes}\")\n",
    "print(f\"Number of unique source nodes: {len(unique_src_nodes)}\")\n",
    "print(f\"Number of unique destination nodes: {len(unique_dst_nodes)}\")\n",
    "# print(f\"Number of unique relations: {len(unique_relations)}\")\n",
    "print(f\"Number of unique timestamps: {len(unique_timestamps)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzGtp9u2pTii",
    "outputId": "29782866-e30d-403d-ab18-e969505e4c31"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.loader import TemporalDataLoader\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv, GCNConv\n",
    "from torch_geometric.nn.dense import dense_diff_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "from torch_geometric.nn.models.tgn import IdentityMessage, LastAggregator, LastNeighborLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "def train_val_test_split(self, val_ratio=0.15, test_ratio=0.15):\n",
    "        \"\"\"\n",
    "        Splits the dataset into train, validation, and test sets.\n",
    "        \"\"\"\n",
    "        np.random.shuffle(self.data)  # Shuffle the data\n",
    "        total_size = len(self.data)\n",
    "        \n",
    "        # Calculate sizes for each split\n",
    "        test_size = int(total_size * test_ratio)\n",
    "        val_size = int(total_size * val_ratio)\n",
    "        \n",
    "        # Perform the split\n",
    "        test_data = self.data[:test_size]\n",
    "        val_data = self.data[test_size:test_size + val_size]\n",
    "        train_data = self.data[test_size + val_size:]\n",
    "        \n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "\n",
    "# Split dataset into train/val/test\n",
    "train_data, val_data, test_data = data.train_val_test_split(val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "print(\"train date\")\n",
    "print(\"Source nodes:\", train_data.src)\n",
    "print(\"Destination nodes:\", train_data.dst)\n",
    "print(\"Number of edges:\", train_data.src.size(0))\n",
    "print(\"======\")\n",
    "print(\"test date\")\n",
    "print(\"Source nodes:\", test_data.src)\n",
    "print(\"Destination nodes:\", test_data.dst)\n",
    "print(\"Number of edges:\", test_data.src.size(0))\n",
    "print(\"======\")\n",
    "print(\"val date\")\n",
    "print(\"Source nodes:\", val_data.src)\n",
    "print(\"Destination nodes:\", val_data.dst)\n",
    "print(\"Number of edges:\", val_data.src.size(0))\n",
    "print(\"======\")\n",
    "\n",
    "if train_data.src.numel() == 0 or train_data.dst.numel() == 0:\n",
    "    print(\"Edge information is empty. Check data loading or preprocessing.\")\n",
    "else:\n",
    "    print(\"Edges are present.\")\n",
    "\n",
    "\n",
    "print(\"Sample edges (src -> dst):\")\n",
    "for i in range(min(10, train_data.src.size(0))):\n",
    "    print(f\"{train_data.src[i].item()} -> {train_data.dst[i].item()}\")\n",
    "\n",
    "\n",
    "# Count edges for each node\n",
    "unique_nodes, edge_counts = torch.cat([train_data.src, train_data.dst]).unique(return_counts=True)\n",
    "print(\"Nodes and their edge counts:\", list(zip(unique_nodes.tolist(), edge_counts.tolist())))\n",
    "\n",
    "\n",
    "\n",
    "# Print the size and number of events in each split\n",
    "print(f'Number of events in each split:')\n",
    "print(f'  - Train: {train_data.num_events}')\n",
    "print(f'  - Validation: {val_data.num_events}')\n",
    "print(f'  - Test: {test_data.num_events}')\n",
    "\n",
    "\n",
    "# Initialize TemporalDataLoader for each dataset split\n",
    "train_loader = TemporalDataLoader(train_data, batch_size=512, neg_sampling_ratio=1.0)\n",
    "val_loader = TemporalDataLoader(val_data, batch_size=512, neg_sampling_ratio=1.0)\n",
    "test_loader = TemporalDataLoader(test_data, batch_size=512, neg_sampling_ratio=1.0)\n",
    "\n",
    "# Initialize LastNeighborLoader for neighborhood sampling\n",
    "neighbor_loader = LastNeighborLoader(num_nodes=data.num_nodes, size=10, device=device)\n",
    "\n",
    "# Checking batches in the data loader\n",
    "# print('====================')\n",
    "# batch_count = 0  # Initialize a counter\n",
    "# print('Check batches in dataloader')\n",
    "# for batch in train_loader:\n",
    "#     print(f\"Batch {batch_count + 1}:\")\n",
    "#     print(batch)  # Check the contents of the batch\n",
    "#     print(batch.src, batch.dst, batch.t, batch.msg)  # Access batch attributes\n",
    "#     batch_count += 1  # Increment the counter\n",
    "#     if batch_count == 2:  # Stop after two batches\n",
    "#         break\n",
    "\n",
    "# print('===================')\n",
    "print('Print neighbor loader')\n",
    "\n",
    "# Get all unique node IDs from 'src' and 'dst' attributes of your data\n",
    "all_node_ids = torch.cat([train_data.src, train_data.dst]).unique()\n",
    "\n",
    "# Pass these unique node IDs to the neighbor loader\n",
    "n_id, edge_index, e_id = neighbor_loader(all_node_ids)\n",
    "\n",
    "# Print the shapes and some values\n",
    "# print(n_id.shape, edge_index.shape, e_id.shape)\n",
    "# print(n_id[:10], edge_index[:, :10], e_id[:10])  # Print some values\n",
    "\n",
    "\n",
    "\n",
    "##check msg size\n",
    "# print(\"data msg size\")\n",
    "# print(data.msg.size())\n",
    "# print(data.msg.shape)\n",
    "\n",
    "# print(f\"data.msg shape: {data.msg.shape} (Expected: [total_num_edges, msg_dim])\")\n",
    "# print(f\"data.msg example values: {data.msg[:5]}\")  # Print a few example values to inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z1tB267hqFel",
    "outputId": "d00903d4-b56c-43f8-fa3d-0ab3bf0f4705"
   },
   "outputs": [],
   "source": [
    "# Utility function for optional debugging output\n",
    "def debug_shapes(**shapes):\n",
    "    for name, value in shapes.items():\n",
    "        print(f\"{name} shape: {value.shape}\")\n",
    "\n",
    "\n",
    "class DiffPoolLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_nodes, ratio=0.25):\n",
    "        super().__init__()\n",
    "        self.num_clusters = max(1, int(ratio * num_nodes))\n",
    "        self.gnn_embed = GCNConv(in_channels, hidden_channels)\n",
    "        self.gnn_pool = GCNConv(in_channels, self.num_clusters)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_channels) # Add batch normalization here\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch, debug=False):\n",
    "        embed = self.gnn_embed(x, edge_index)\n",
    "        embed = self.bn(embed)  # Add batch normalization here\n",
    "        s = self.gnn_pool(x, edge_index).softmax(dim=-1)\n",
    "\n",
    "        # Convert to dense batch\n",
    "        x, mask = to_dense_batch(embed, batch)\n",
    "        adj = to_dense_adj(edge_index, batch)\n",
    "        s, _ = to_dense_batch(s, batch)\n",
    "\n",
    "        if debug:\n",
    "            debug_shapes(x=x, adj=adj, s=s)\n",
    "\n",
    "        # Differential pooling\n",
    "        x, adj, link_loss, ent_loss = dense_diff_pool(x, adj, s, mask, normalize=True)\n",
    "        x = x.view(-1, x.size(-1))  # Flatten the output if necessary\n",
    "\n",
    "        return x, adj, link_loss, ent_loss\n",
    "\n",
    "\n",
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super().__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = TransformerConv(in_channels, out_channels // 2, heads=2, dropout=0.1, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        # print(\"Debugging GraphAttentionEmbedding:\")\n",
    "        # print(f\"  x shape: {x.shape}\")\n",
    "        # print(f\"  last_update shape: {last_update.shape}\")\n",
    "        # print(f\"  edge_index shape: {edge_index.shape}\")\n",
    "        # print(f\"  t shape: {t.shape}\")\n",
    "        # print(f\"  msg shape: {msg.shape}\")\n",
    "        # print(f\"  edge_index content: {edge_index}\")\n",
    "        # print(f\"  edge_attr content before conv: {torch.cat([self.time_enc(last_update[edge_index[0]] - t.to(x.dtype)), msg], dim=-1)}\")\n",
    "\n",
    "\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        return self.conv(x, edge_index, edge_attr)\n",
    "\n",
    "\n",
    "class FeatureFusion(torch.nn.Module):\n",
    "    def __init__(self, tgn_dim, diffpool_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tgn_projection = Linear(tgn_dim, output_dim)\n",
    "        self.diffpool_projection = Linear(diffpool_dim, output_dim)\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim=output_dim, num_heads=4, batch_first=True)\n",
    "        self.final_projection = Linear(output_dim, output_dim)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, tgn_features, diffpool_features):\n",
    "        # tgn_proj = self.tgn_projection(tgn_features)\n",
    "        # diffpool_proj = self.diffpool_projection(diffpool_features)\n",
    "        tgn_proj = self.dropout(self.tgn_projection(tgn_features))\n",
    "        diffpool_proj = self.dropout(self.diffpool_projection(diffpool_features))\n",
    "\n",
    "        diffpool_proj = F.interpolate(diffpool_proj.unsqueeze(0).unsqueeze(0),\n",
    "                                      size=(tgn_proj.size(0), diffpool_proj.size(1)),\n",
    "                                      mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "        combined = tgn_proj + diffpool_proj\n",
    "        attended, _ = self.attention(combined.unsqueeze(0), combined.unsqueeze(0), combined.unsqueeze(0))\n",
    "        return self.final_projection(attended.squeeze(0))\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels)\n",
    "        self.lin_dst = Linear(in_channels, in_channels)\n",
    "        self.lin_final = Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = (self.lin_src(z_src) * self.lin_dst(z_dst)).relu()\n",
    "        return self.lin_final(h)\n",
    "\n",
    "\n",
    "# Model dimensions\n",
    "memory_dim = time_dim = embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "final_dim = 128\n",
    "\n",
    "\n",
    "# print(\"data msg size\")\n",
    "# print(data.msg.size())\n",
    "# print(data.msg.shape)\n",
    "\n",
    "# print(f\"data.msg shape: {data.msg.shape} (Expected: [total_num_edges, msg_dim])\")\n",
    "# print(f\"data.msg example values: {data.msg[:5]}\")  # Print a few example values to inspect\n",
    "\n",
    "# Initialize models\n",
    "memory = TGNMemory(data.num_nodes, data.msg.size(-1), memory_dim, time_dim,\n",
    "                   message_module=IdentityMessage(data.msg.size(-1), memory_dim, time_dim),\n",
    "                   aggregator_module=LastAggregator()).to(device)\n",
    "\n",
    "# print(\"Initializing GraphAttentionEmbedding with the following parameters:\")\n",
    "# print(f\"  memory_dim (in_channels): {memory_dim} (Expected: integer, dimension size for input features)\")\n",
    "# print(f\"  embedding_dim (out_channels): {embedding_dim} (Expected: integer, dimension size for output features)\")\n",
    "# print(f\"  Message dimension (data.msg.size(-1)): {data.msg.size(-1)} (Expected: integer, size of each message embedding)\")\n",
    "# print(f\"  Time encoder (memory.time_enc): {memory.time_enc} (Expected: instance of TimeEncoder or similar)\")\n",
    "\n",
    "\n",
    "gnn = GraphAttentionEmbedding(memory_dim, embedding_dim, data.msg.size(-1), memory.time_enc).to(device)\n",
    "\n",
    "diffpool = DiffPoolLayer(memory_dim, hidden_dim, data.num_nodes, ratio=0.3).to(device)\n",
    "\n",
    "fusion = FeatureFusion(embedding_dim, hidden_dim, final_dim).to(device)\n",
    "\n",
    "link_pred = LinkPredictor(final_dim).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(list(memory.parameters()) + list(gnn.parameters()) +\n",
    "                             list(diffpool.parameters()) + list(fusion.parameters()) +\n",
    "                             list(link_pred.parameters()), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0], device=device))\n",
    "\n",
    "# Helper vector for node mapping\n",
    "assoc = torch.empty(data.num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(batch, debug=False):\n",
    "    # Loading neighbors and setting up the association vector\n",
    "    # print(\"\\nLoading neighbors for batch...\")\n",
    "    n_id, edge_index, e_id = neighbor_loader(batch.n_id)\n",
    "    # print(f\"e_id shape: {e_id.shape} (Expected: 1D tensor with indices)\")\n",
    "    # print(f\"e_id content: {e_id[:10]}\")  # Print the first few indices for inspection\n",
    "\n",
    "\n",
    "    # Updating association vector\n",
    "    assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "\n",
    "    # Initializing batch_vector\n",
    "    batch_vector = torch.zeros(n_id.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    # TGN Embedding\n",
    "    z, last_update = memory(n_id)\n",
    "\n",
    "    # print(\"Debugging inputs to GraphAttentionEmbedding forward pass:\")\n",
    "    # print(f\"  z shape: {z.shape} (Expected: [num_nodes, memory_dim])\")\n",
    "    # print(f\"  last_update shape: {last_update.shape} (Expected: [num_nodes])\")\n",
    "    # print(f\"  edge_index shape: {edge_index.shape} (Expected: [2, num_edges])\")\n",
    "    # print(f\"  data.t[e_id].shape: {data.t[e_id].to(device).shape} (Expected: [num_edges])\")\n",
    "    # print(f\"  data.msg[e_id].shape: {data.msg[e_id].to(device).shape} (Expected: [num_edges, msg_dim])\")\n",
    "    # print(f\"e_id shape: {e_id.shape} (Expected: 1D tensor with indices)\")\n",
    "    # print(f\"e_id content: {e_id[:10]}\")  # Print the first few indices for inspection\n",
    "    # print(f\"data.msg[e_id].shape: {data.msg[e_id].shape}\")\n",
    "    # print(f\"data.msg[e_id] example values: {data.msg[e_id][:5]}\")  # Inspect a few values if needed\n",
    "\n",
    "\n",
    "    z_tgn = gnn(z, last_update, edge_index, data.t[e_id].to(device), data.msg[e_id].to(device))\n",
    "\n",
    "\n",
    "    # DiffPool Embedding\n",
    "    # print(\"\\nGenerating DiffPool embedding...\")\n",
    "    z_diffpool, adj_diffpool, link_loss, ent_loss = diffpool(z, edge_index, batch_vector, debug=debug)\n",
    "    # print(\"DiffPool output z_diffpool:\", z_diffpool)\n",
    "    # print(\"DiffPool output adj_diffpool:\", adj_diffpool)\n",
    "    # print(\"Link loss:\", link_loss)\n",
    "    # print(\"Entropy loss:\", ent_loss)\n",
    "\n",
    "    # Feature Fusion\n",
    "    # print(\"\\nFusing TGN and DiffPool features...\")\n",
    "    z_combined = fusion(z_tgn, z_diffpool)\n",
    "    # print(\"Combined feature vector z_combined:\", z_combined)\n",
    "    return z_combined, link_loss, ent_loss\n",
    "\n",
    "\n",
    "def train_epoch():\n",
    "   # Set models to training mode and confirm\n",
    "\n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    diffpool.train()\n",
    "    fusion.train()\n",
    "    link_pred.train()\n",
    "    memory.reset_state()\n",
    "    neighbor_loader.reset_state()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        #print(f\"Processing batch {batch_idx + 1}...\")  # Print the batch number (1-based index)\n",
    "\n",
    "        # if batch.edge_index.size(1) == 0:  # Skip batch if no edges are present\n",
    "        #     print(\"Skipping empty batch.\")\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     print(f\"Processing batch {batch_idx + 1} with {batch.edge_index.size(1)} edges.\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        z_combined, link_loss, ent_loss = process_batch(batch, debug=False)\n",
    "        pos_out = link_pred(z_combined[assoc[batch.src]], z_combined[assoc[batch.dst]])\n",
    "        neg_out = link_pred(z_combined[assoc[batch.src]], z_combined[assoc[batch.neg_dst]])\n",
    "\n",
    "        # Debugging: Print positive and negative scores\n",
    "        # print(\"Positive Scores (pos_out):\", pos_out[:5].detach().cpu().numpy())\n",
    "        # print(\"Negative Scores (neg_out):\", neg_out[:5].detach().cpu().numpy())\n",
    "\n",
    "        # Debugging: Visualize score distributions\n",
    "        # plt.hist(pos_out.detach().cpu().numpy(), bins=50, alpha=0.5, label=\"Positive Scores\")\n",
    "        # plt.hist(neg_out.detach().cpu().numpy(), bins=50, alpha=0.5, label=\"Negative Scores\")\n",
    "        # plt.legend()\n",
    "        # plt.title(\"Distribution of Positive and Negative Scores\")\n",
    "        # plt.show()\n",
    "\n",
    "        loss = criterion(pos_out, torch.ones_like(pos_out))\n",
    "        loss += criterion(neg_out, torch.zeros_like(neg_out))\n",
    "        loss += 0.1 * (link_loss + ent_loss)\n",
    "\n",
    "        memory.update_state(batch.src, batch.dst, batch.t, batch.msg)\n",
    "        neighbor_loader.insert(batch.src, batch.dst)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "\n",
    "    return total_loss / train_data.num_events\n",
    "\n",
    "# def calculate_mrr(pos_out, neg_out):\n",
    "#     # Combine scores\n",
    "#     all_scores = torch.cat([pos_out, neg_out], dim=0)\n",
    "#     labels = torch.cat([torch.ones(pos_out.size(0)), torch.zeros(neg_out.size(0))], dim=0)\n",
    "#     labels = labels.to(all_scores.device)\n",
    "\n",
    "#     # Step 2: Debugging label distribution\n",
    "#     print(\"Labels (1=Positive, 0=Negative):\", labels.detach().cpu().numpy())\n",
    "#     print(f\"Number of Positive Labels: {torch.sum(labels == 1).item()}\")\n",
    "#     print(f\"Number of Negative Labels: {torch.sum(labels == 0).item()}\")\n",
    "\n",
    "#     # Sort scores and labels\n",
    "#     sorted_indices = torch.argsort(all_scores, descending=True)\n",
    "#     sorted_labels = labels[sorted_indices]\n",
    "\n",
    "#     # Step 3: Debugging sorted results\n",
    "#     print(\"All Scores (sorted):\", all_scores[sorted_indices][:10].detach().cpu().numpy())\n",
    "#     print(\"Sorted Labels (top 10):\", sorted_labels[:10].detach().cpu().numpy())\n",
    "\n",
    "#     ranks = torch.where(sorted_labels == 1)[0] + 1  # Convert to 1-based rank\n",
    "#     if ranks.size(0) == 0:\n",
    "#         print(\"No positive labels found in sorted labels!\")\n",
    "#         return 0.0\n",
    "\n",
    "#     # reciprocal_ranks = 1.0 / ranks.float()\n",
    "#     ranks = torch.where(sorted_labels == 1)[0] + 1\n",
    "#     print(f\"Ranks of Positive Samples: {ranks.cpu().numpy()}\")\n",
    "#     reciprocal_ranks = 1.0 / ranks.float()\n",
    "#     print(f\"Reciprocal Ranks: {reciprocal_ranks.cpu().numpy()}\")\n",
    "\n",
    "#     return reciprocal_ranks.mean().item()\n",
    "\n",
    "def calculate_mrr(pos_score, neg_score):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR) for a batch of queries.\n",
    "    Args:\n",
    "        pos_score: Tensor of shape [batch_size, 1], scores for positive examples.\n",
    "        neg_score: Tensor of shape [batch_size, num_negatives], scores for negative examples.\n",
    "    Returns:\n",
    "        MRR score (float)\n",
    "    \"\"\"\n",
    "    # Validate input shapes\n",
    "    if pos_score.size(0) != neg_score.size(0):\n",
    "        raise ValueError(\"pos_score and neg_score must have the same batch size.\")\n",
    "\n",
    "    # Combine positive and negative scores for ranking\n",
    "    scores = torch.cat([pos_score, neg_score], dim=1)  # Shape: [batch_size, 1 + num_negatives]\n",
    "\n",
    "    # Create binary labels (1 for positive, 0 for negatives)\n",
    "    labels = torch.zeros_like(scores)\n",
    "    labels[:, 0] = 1  # First column contains positive scores\n",
    "\n",
    "    # Sort scores in descending order and get indices\n",
    "    _, indices = scores.sort(dim=1, descending=True)\n",
    "\n",
    "    # Get ranks of positive samples\n",
    "    positive_indices = indices == 0  # Locate positive samples in sorted indices\n",
    "    ranks = (positive_indices.nonzero(as_tuple=False)[:, 1] + 1).float()  # 1-based rank\n",
    "\n",
    "    # Calculate MRR\n",
    "    mrr = (1.0 / ranks).mean()\n",
    "\n",
    "    return mrr.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "# def evaluate(loader):\n",
    "#     memory.eval(), gnn.eval(), diffpool.eval(), fusion.eval(), link_pred.eval()\n",
    "#     torch.manual_seed(12345)\n",
    "\n",
    "#     aps, aucs, mrrs = [], [], []\n",
    "#     for batch in loader:\n",
    "#         batch = batch.to(device)\n",
    "#         z_combined, _, _ = process_batch(batch)\n",
    "#         pos_out = link_pred(z_combined[assoc[batch.src]], z_combined[assoc[batch.dst]])\n",
    "#         neg_out = link_pred(z_combined[assoc[batch.src]], z_combined[assoc[batch.neg_dst]])\n",
    "\n",
    "\n",
    "\n",
    "#         #  Calculate MRR\n",
    "#         mrr = calculate_mrr(pos_out, neg_out)\n",
    "#         mrrs.append(mrr)\n",
    "\n",
    "#         # Other metrics\n",
    "#         y_pred = torch.cat([pos_out, neg_out], dim=0).sigmoid().cpu()\n",
    "#         y_true = torch.cat([torch.ones(pos_out.size(0)), torch.zeros(neg_out.size(0))], dim=0)\n",
    "#         aps.append(average_precision_score(y_true, y_pred))\n",
    "#         aucs.append(roc_auc_score(y_true, y_pred))\n",
    "\n",
    "#         memory.update_state(batch.src, batch.dst, batch.t, batch.msg)\n",
    "#         neighbor_loader.insert(batch.src, batch.dst)\n",
    "\n",
    "#     return float(torch.tensor(aps).mean()), float(torch.tensor(aucs).mean()), float(torch.tensor(mrrs).mean())\n",
    "\n",
    "def evaluate(loader):\n",
    "    memory.eval(), gnn.eval(), diffpool.eval(), fusion.eval(), link_pred.eval()\n",
    "    torch.manual_seed(12345)\n",
    "\n",
    "    aps, aucs, mrrs = [], [], []\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        z_combined, _, _ = process_batch(batch)\n",
    "        pos_out = link_pred(z_combined[assoc[batch.src]], z_combined[assoc[batch.dst]])\n",
    "        neg_out = link_pred(z_combined[assoc[batch.src]], z_combined[assoc[batch.neg_dst]])\n",
    "\n",
    "        # Calculate loss\n",
    "        pos_target = torch.ones(pos_out.size(0), device=device)\n",
    "        neg_target = torch.zeros(neg_out.size(0), device=device)\n",
    "\n",
    "        # Combine positive and negative samples\n",
    "        all_outputs = torch.cat([pos_out, neg_out], dim=0)\n",
    "        all_targets = torch.cat([pos_target, neg_target], dim=0)\n",
    "\n",
    "        # Calculate loss using the same criterion as in training\n",
    "        loss = criterion(all_outputs, all_targets.view(-1, 1))  # Make sure to use the same criterion as training\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Calculate MRR\n",
    "        mrr = calculate_mrr(pos_out, neg_out)\n",
    "        mrrs.append(mrr)\n",
    "\n",
    "        # Other metrics\n",
    "        y_pred = torch.cat([pos_out, neg_out], dim=0).sigmoid().cpu()\n",
    "        y_true = torch.cat([torch.ones(pos_out.size(0)), torch.zeros(neg_out.size(0))], dim=0)\n",
    "        aps.append(average_precision_score(y_true, y_pred))\n",
    "        aucs.append(roc_auc_score(y_true, y_pred))\n",
    "\n",
    "        memory.update_state(batch.src, batch.dst, batch.t, batch.msg)\n",
    "        neighbor_loader.insert(batch.src, batch.dst)\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return (avg_loss,\n",
    "            float(torch.tensor(aps).mean()),\n",
    "            float(torch.tensor(aucs).mean()),\n",
    "            float(torch.tensor(mrrs).mean()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop with early stopping\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses, val_losses, val_aucs, test_aucs, val_mrrs, test_mrrs = [], [], [], [], [], []\n",
    "\n",
    "best_val_auc = 0\n",
    "early_stopping_patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "#scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "# Checkpoint and early stopping fix\n",
    "for epoch in range(1, 100): #151\n",
    "    print(f\"Epoch {epoch}/{150}\")  # You can replace 150 with the total number of epochs if it changes\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch()\n",
    "    # val_ap, val_auc, val_mrr = evaluate(val_loader)\n",
    "    # test_ap, test_auc, test_mrr = evaluate(test_loader)\n",
    "\n",
    "\n",
    "    # # Track metrics\n",
    "    # train_losses.append(train_loss)\n",
    "    # val_losses.append(val_auc)\n",
    "    # val_aucs.append(val_auc)\n",
    "    # test_aucs.append(test_auc)\n",
    "    # val_mrrs.append(val_mrr)  # Append validation MRR\n",
    "    # test_mrrs.append(test_mrr)  # Append test MRR\n",
    "    val_loss, val_ap, val_auc, val_mrr = evaluate(val_loader)\n",
    "    test_loss, test_ap, test_auc, test_mrr = evaluate(test_loader)\n",
    "\n",
    "    # Track metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)  # Now tracking actual validation loss\n",
    "    val_aucs.append(val_auc)\n",
    "    test_aucs.append(test_auc)\n",
    "    val_mrrs.append(val_mrr)\n",
    "    test_mrrs.append(test_mrr)\n",
    "\n",
    "    #print(f'Epoch {epoch:02d}, Loss: {train_loss:.4f}, Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f}, Val MRR: {val_mrr:.4f}, Test AUC: {test_auc:.4f}, Test MRR: {test_mrr:.4f}')\n",
    "    print(f\"Epoch :{epoch:02d}\")\n",
    "    print(f'Loss: {train_loss:.4f}')\n",
    "    print(f\"Validation AP: {val_ap:.4f}, Validation AUC: {val_auc:.4f}, Validation MRR: {val_mrr:.4f}\")\n",
    "    print(f\"Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}, Test MRR: {test_mrr:.4f}\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "      print(f\"Learning rate: {param_group['lr']}\")\n",
    "    print(\"================\")\n",
    "\n",
    "    # Step the scheduler with the validation AUC\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "\n",
    "    #Check for early stopping\n",
    "    if val_auc > best_val_auc:\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "        best_val_auc = val_auc\n",
    "        best_val_ap = val_ap  # Track the best AP\n",
    "        best_val_mrr = val_mrr  # Track the best MRR\n",
    "\n",
    "        # Save the checkpoint\n",
    "        torch.save({\n",
    "            'memory': memory.state_dict(),\n",
    "            'gnn': gnn.state_dict(),\n",
    "            'diffpool': diffpool.state_dict(),\n",
    "            'fusion': fusion.state_dict(),\n",
    "            'link_pred': link_pred.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_auc': val_auc,\n",
    "            'val_ap': val_ap,\n",
    "            'val_mrr': val_mrr,\n",
    "            'test_auc': test_auc,\n",
    "            'test_ap': test_ap,\n",
    "            'test_mrr': test_mrr,\n",
    "        }, f\"model_checkpoint_epoch_{epoch}.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch} completed in {epoch_time:.2f}s\")\n",
    "\n",
    "\n",
    "# Plotting Loss and AUC over Epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training and Validation Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.savefig(\"train_loss1.png\")  # Save loss plot\n",
    "\n",
    "# Plot MRR\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_mrrs, label='Val MRR')\n",
    "plt.plot(test_mrrs, label='Test MRR')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MRR')\n",
    "plt.legend()\n",
    "plt.title('Test MRR')\n",
    "\n",
    "# Validation and Test AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(val_aucs, label='Val AUC')\n",
    "plt.plot(test_aucs, label='Test AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.title('Test AUC')\n",
    "plt.savefig(\"test_auc1.png\")  # Save AUC plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
